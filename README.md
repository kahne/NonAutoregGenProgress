Non-Autoregressive Generation Progress
======

### 2020
- [arXiv] [BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining](https://arxiv.org/pdf/2012.15525.pdf)
- [arXiv] [Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade](https://arxiv.org/pdf/2012.15833.pdf)
- [arXiv] [Non-Autoregressive Neural Dialogue Generation](https://arxiv.org/pdf/2002.04250.pdf)
- [arXiv] [Improving Fluency of Non-Autoregressive Machine Translation](https://arxiv.org/pdf/2004.03227.pdf)
- [arXiv] [GLAT: Glancing Transformer for Non-Autoregressive Neural Machine Translation](https://arxiv.org/pdf/2008.07905.pdf)
- [arXiv] [Task-Level Curriculum Learning for Non-Autoregressive Neural Machine Translation](https://arxiv.org/pdf/2007.08772.pdf)
- [arXiv] [Insertion-Based Modeling for End-to-End Automatic Speech Recognition](https://arxiv.org/pdf/2005.13211.pdf)
- [arXiv] [Semi-Autoregressive Training Improves Mask-Predict Decoding](https://arxiv.org/pdf/2001.08785.pdf)
- [arXiv] [LAVA NAT: A Non-Autoregressive Translation Model with Look-Around Decoding and Vocabulary Attention](https://arxiv.org/pdf/2002.03084.pdf)
- [COLING] [Context-Aware Cross-Attention for Non-Autoregressive Translation](https://arxiv.org/abs/2011.00770)
- [NeurIPS] [Incorporating BERT into Parallel Sequence Decoding with Adapters](https://arxiv.org/pdf/2010.06138.pdf)
- [EMNLP] [Non-Autoregressive Machine Translation with Latent Alignments](https://arxiv.org/pdf/2004.07437.pdf)
- [EMNLP] [Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation](https://arxiv.org/pdf/2009.07177.pdf)
- [INTERSPEECH] [Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict](https://arxiv.org/pdf/2005.08700.pdf)
- [ACL] [Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation](https://www.aclweb.org/anthology/2020.acl-main.277.pdf)
- [ACL] [Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation](https://www.aclweb.org/anthology/2020.acl-main.36.pdf)
- [ACL] [ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation](https://www.aclweb.org/anthology/2020.acl-main.251.pdf)
- [ACL] [Improving Non-autoregressive Neural Machine Translation with Monolingual Data](https://www.aclweb.org/anthology/2020.acl-main.171.pdf)
- [ACL] [A Study of Non-autoregressive Model for Sequence Generation](https://www.aclweb.org/anthology/2020.acl-main.15.pdf)
- [ICML] [Non-Autoregressive Neural Text-to-Speech](https://arxiv.org/pdf/1905.08459.pdf)
- [ICML] [Aligned Cross Entropy for Non-Autoregressive Machine Translation](https://arxiv.org/pdf/2004.01655.pdf)
- [ICML] [Parallel Machine Translation with Disentangled Context Transformer](https://arxiv.org/pdf/2001.05136.pdf)
- [ICML] [Imputer: Sequence Modelling via Imputation and Dynamic Programming](https://arxiv.org/pdf/2002.08926.pdf)
- [ICML] [An EM Approach to Non-autoregressive Conditional Sequence Generation](https://arxiv.org/pdf/2006.16378.pdf)
- [ICLR] [Understanding Knowledge Distillation in Non-autoregressive Machine Translation](https://arxiv.org/pdf/1911.02727.pdf)
- [AAAI] [Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation](https://arxiv.org/pdf/1911.09320.pdf)
- [AAAI] [Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference Using a Delta Posterior](https://arxiv.org/pdf/1908.07181.pdf)
- [AAAI] [Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation](https://arxiv.org/pdf/1911.08717.pdf)

### 2019
- [arXiv] [Non-autoregressive Transformer by Position Learning](https://arxiv.org/pdf/1911.10677.pdf)
- [arXiv] [Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information](https://arxiv.org/pdf/1911.02215.pdf)
- [arXiv] [Non-Autoregressive Transformer Automatic Speech Recognition](https://arxiv.org/pdf/1911.04908.pdf)
- [NeurIPS] [Levenshtein Transformer](https://papers.nips.cc/paper/9297-levenshtein-transformer.pdf)
- [NeurIPS] [Fast Structured Decoding for Sequence Models](https://arxiv.org/pdf/1910.11555.pdf)
- [EMNLP] [Mask-Predict: Parallel Decoding of Conditional Masked Language Models](https://arxiv.org/pdf/1904.09324.pdf)
- [EMNLP] [FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow](https://arxiv.org/pdf/1909.02480.pdf)
- [EMNLP] [Hint-Based Training for Non-Autoregressive Machine Translation](https://www.aclweb.org/anthology/D19-1573.pdf)
- [ACL] [Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation](https://arxiv.org/pdf/1906.09444.pdf)
- [ACL] [Imitation Learning for Non-Autoregressive Neural Machine Translation](https://arxiv.org/pdf/1906.02041.pdf)
- [AAAI] [Non-Autoregressive Machine Translation with Auxiliary Regularization](https://arxiv.org/pdf/1902.10245.pdf)
- [AAAI] [Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input](https://arxiv.org/pdf/1812.09664.pdf)

### 2018
- [EMNLP] [Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement](https://arxiv.org/pdf/1802.06901.pdf)
- [EMNLP] [End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification](https://arxiv.org/pdf/1811.04719.pdf)
- [ICLR] [Non-Autoregressive Neural Machine Translation](https://arxiv.org/pdf/1711.02281.pdf)

# Contact
Changhan Wang ([wangchanghan@gmail.com](mailto:wangchanghan@gmail.com))
